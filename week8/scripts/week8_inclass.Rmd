---
title: "week 8 in class"
author: "Diana Gao"
date: "2024-10-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Load libraries

```{r}
library(here)
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(janeaustenr)
library(stopwords)
```

## Think Pair Share

Let's clean it up. Lets replace all the "." with "-" and extract only the numbers (leaving the letters behind). Remove any extra white space. You can use a sequence of pipes.

```{r}
strings<-c("550-153-7578",
         "banana",
         "435.114.7586",
         "home: 672-442-6739") # list of answers to give me ur phoen number survey
phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"
test<-str_subset(strings, phone) # subset only the actual phone numbers

cleaned <- test %>%
  str_replace_all("\\.","\\-") %>%
  str_replace_all("[a-z]", " ") %>%
  str_replace_all(":", " ") %>%
  str_trim()


```

## Jane Austen Stuffs
```{r}
#head(austen_books())

original_books <- austen_books() %>% # get all of Jane Austen's books
  group_by(book) %>%
  mutate(line = row_number(), # find every line
        chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", # count the chapters.cumsum() is cumulative sum, puts the chapters in order by counting number of trues. For the regex: starts with the word chapter followed by a digit (\\d) or roman numeral(ivxlc))
        ignore_case = TRUE)))) %>% #ignore lower or uppercase
  ungroup() # ungroup it so we have a dataframe again
# don't try to view the entire thing... its >73000 lines...
#head(original_books)
tidy_books <- original_books %>%
  unnest_tokens(output = word, input = text) # add a column named word, with the input as the text column
head(tidy_books) # there are now >725,000 rows. Don't view the entire thing!
```
## Now select only the important words
```{r}
head(get_stopwords()) #see an example of all the stopwords
cleaned_books <- tidy_books %>%
  anti_join(get_stopwords()) # dataframe without the stopwords
```
## Count how many times each word appears with stop words removed
```{r}
cleaned_books %>%
  count(word, sort = TRUE) # wow mr and mrs is most well used haha
head(cleaned_books)
```

## Sentimental analysis
```{r}
sent_word_counts <- tidy_books %>%
  inner_join(get_sentiments()) %>% # only keep pos or negative words. Can replace this with whatever specific text that you want to keep
  count(word, sentiment, sort = TRUE) # count them
#head(sent_word_counts)[1:3,]

sent_word_counts %>%
  filter(n > 150) %>% # take only if there are over 150 instances of it
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>% # add a column where if the word is negative make the count negative
  mutate(word = reorder(word, n)) %>% # sort it so it gows from largest to smallest
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```

## Make a word cloud! 
```{r}
words<-cleaned_books %>%
  count(word) %>% # count all the words
  arrange(desc(n))%>% # sort the words
  slice(1:100) #take the top 100
wordcloud2(words, shape = 'triangle', size=0.3) # make a wordcloud out of the top 100 words in the shape of a triangle
```


